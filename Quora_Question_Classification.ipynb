{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17423878f5d9c2c469700a932b708ed106f0d7ae"
   },
   "source": [
    "## Quora Question Classification Challenge - Kaggle Competition.\n",
    "\n",
    "In this notebook, I have worked around a method to classify whether the question is sincere or insincere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import all the packages I have used ahead in one go so to keep the code clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, Embedding, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the progress bar and get the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "train_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make a list from the pandas column. Using this manner, it is quite easy to process the sentences for using ahead to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "123f60c096fd54baa4d02ac6f0f2a04fc7b4365b"
   },
   "outputs": [],
   "source": [
    "sentence_list_train = train_data['question_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing a code to various function makes our code clean. So let's do the same now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15c81696c0183a728862735430eea17c6cdcbece"
   },
   "outputs": [],
   "source": [
    "def handle_punctuations(sentence):\n",
    "    '''\n",
    "    To handle the punctuations, we replace '&' with 'and' and for others, we just remove it. The first\n",
    "    loop is deprecated and redundant and is never run because condition is never met. It is just there\n",
    "    because previous version of the code was using it.\n",
    "    '''\n",
    "    sentence = str(sentence)\n",
    "    for punct in \"/-'\":\n",
    "        sentence = sentence.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        sentence = sentence.replace(punct, ' and ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        sentence = sentence.replace(punct, '')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6356f37b0391f5a4e5954c4ffe421e7dae99776"
   },
   "outputs": [],
   "source": [
    "def handle_contractions(sentence):\n",
    "    '''\n",
    "    We need to handle contractions. To do that, a manually created dictionary is used. Whenever a contraction\n",
    "    is encountered, it searches for it in keys of the dictionary, and replaces it with its value in the \n",
    "    sentence. Need to do so is to ensure most of the data actually has an embedding present so that Keras\n",
    "    model can map word to vector.\n",
    "    '''\n",
    "    contraction_dict = {\"ain't\": \"am not\",\n",
    "                        \"aren't\": \"are not\",\n",
    "                        \"can't\": \"cannot\",\n",
    "                        \"can't've\": \"cannot have\",\n",
    "                        \"'cause\": \"because\",\n",
    "                        \"could've\": \"could have\",\n",
    "                        \"couldn't\": \"could not\",\n",
    "                        \"couldn't've\": \"could not have\",\n",
    "                        \"didn't\": \"did not\",\n",
    "                        \"doesn't\": \"does not\",\n",
    "                        \"Don't\": \"do not\",\n",
    "                        \"don't\": \"do not\",\n",
    "                        \"hadn't\": \"had not\",\n",
    "                        \"hadn't've\": \"had not have\",\n",
    "                        \"hasn't\": \"has not\",\n",
    "                        \"haven't\": \"have not\",\n",
    "                        \"he'd\": \"he would\",\n",
    "                        \"he'd've\": \"he would have\",\n",
    "                        \"he'll\": \"he will\",\n",
    "                        \"he'll've\": \"he will have\",\n",
    "                        \"he's\": \"he is\",\n",
    "                        \"how'd\": \"how did\",\n",
    "                        \"how'd'y\": \"how do you\",\n",
    "                        \"how'll\": \"how will\",\n",
    "                        \"how's\": \"how is\",\n",
    "                        \"I'd\": \"I would\",\n",
    "                        \"I'd've\": \"I would have\",\n",
    "                        \"I'll\": \"I will\",\n",
    "                        \"I'll've\": \"I will have\",\n",
    "                        \"i'm\": \"I am\",\n",
    "                        \"I'm\": \"I am\",\n",
    "                        \"I've\": \"I have\",\n",
    "                        \"isn't\": \"is not\",\n",
    "                        \"it'd\": \"it had\",\n",
    "                        \"it'd've\": \"it would have\",\n",
    "                        \"it'll\": \"it will\",\n",
    "                        \"it'll've\": \"it will have\",\n",
    "                        \"it's\": \"it is\",\n",
    "                        \"let's\": \"let us\",\n",
    "                        \"ma'am\": \"madam\",\n",
    "                        \"mayn't\": \"may not\",\n",
    "                        \"might've\": \"might have\",\n",
    "                        \"mightn't\": \"might not\",\n",
    "                        \"mightn't've\": \"might not have\", \n",
    "                        \"must've\": \"must have\",\n",
    "                        \"mustn't\": \"must not\",\n",
    "                        \"mustn't've\": \"must not have\",\n",
    "                        \"needn't\": \"need not\",\n",
    "                        \"needn't've\": \"need not have\",\n",
    "                        \"o'clock\": \"of the clock\",\n",
    "                        \"oughtn't\": \"ought not\",\n",
    "                        \"oughtn't've\": \"ought not have\",\n",
    "                        \"shan't\": \"shall not\",\n",
    "                        \"sha'n't\": \"shall not\",\n",
    "                        \"shan't've\": \"shall not have\",\n",
    "                        \"she'd\": \"she would\",\n",
    "                        \"she'd've\": \"she would have\",\n",
    "                        \"she'll\": \"she will\",\n",
    "                        \"she'll've\": \"she will have\",\n",
    "                        \"she's\": \"she is\",\n",
    "                        \"should've\": \"should have\",\n",
    "                        \"shouldn't\": \"should not\",\n",
    "                        \"shouldn't've\": \"should not have\",\n",
    "                        \"so've\": \"so have\",\n",
    "                        \"so's\": \"so is\",\n",
    "                        \"that'd\": \"that would\",\n",
    "                        \"that'd've\": \"that would have\",\n",
    "                        \"that's\": \"that is\",\n",
    "                        \"there'd\": \"there had\",\n",
    "                        \"there'd've\": \"there would have\",\n",
    "                        \"there's\": \"there is\",\n",
    "                        \"they'd\": \"they would\",\n",
    "                        \"they'd've\": \"they would have\",\n",
    "                        \"they'll\": \"they will\",\n",
    "                        \"they'll've\": \"they will have\",\n",
    "                        \"they're\": \"they are\",\n",
    "                        \"they've\": \"they have\",\n",
    "                        \"to've\": \"to have\",\n",
    "                        \"wasn't\": \"was not\",\n",
    "                        \"we'd\": \"we had\",\n",
    "                        \"we'd've\": \"we would have\",\n",
    "                        \"we'll\": \"we will\",\n",
    "                        \"we'll've\": \"we will have\",\n",
    "                        \"we're\": \"we are\",\n",
    "                        \"we've\": \"we have\",\n",
    "                        \"weren't\": \"were not\",\n",
    "                        \"what'll\": \"what will\",\n",
    "                        \"what'll've\": \"what will have\",\n",
    "                        \"what're\": \"what are\",\n",
    "                        \"what's\": \"what is\",\n",
    "                        \"what've\": \"what have\",\n",
    "                        \"when's\": \"when is\",\n",
    "                        \"when've\": \"when have\",\n",
    "                        \"where'd\": \"where did\",\n",
    "                        \"where's\": \"where is\",\n",
    "                        \"where've\": \"where have\",\n",
    "                        \"who'll\": \"who will\",\n",
    "                        \"who'll've\": \"who will have\",\n",
    "                        \"who's\": \"who is\",\n",
    "                        \"who've\": \"who have\",\n",
    "                        \"why's\": \"why is\",\n",
    "                        \"why've\": \"why have\",\n",
    "                        \"will've\": \"will have\",\n",
    "                        \"won't\": \"will not\",\n",
    "                        \"won't've\": \"will not have\",\n",
    "                        \"would've\": \"would have\",\n",
    "                        \"wouldn't\": \"would not\",\n",
    "                        \"wouldn't've\": \"would not have\",\n",
    "                        \"y'all\": \"you all\",\n",
    "                        \"y'alls\": \"you alls\",\n",
    "                        \"y'all'd\": \"you all would\",\n",
    "                        \"y'all'd've\": \"you all would have\",\n",
    "                        \"y'all're\": \"you all are\",\n",
    "                        \"y'all've\": \"you all have\",\n",
    "                        \"you'd\": \"you had\",\n",
    "                        \"you'd've\": \"you would have\",\n",
    "                        \"you'll\": \"you you will\",\n",
    "                        \"you'll've\": \"you you will have\",\n",
    "                        \"you're\": \"you are\",\n",
    "                        \"you've\": \"you have\"\n",
    "                       }\n",
    "    updated_sentence = \"\"\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            updated_sentence += contraction_dict[word]\n",
    "        except KeyError:\n",
    "            updated_sentence += word\n",
    "        updated_sentence += \" \"\n",
    "    return updated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4417f8957a1faf95c9ef73ef5cc7a3c692b7450"
   },
   "outputs": [],
   "source": [
    "def handle_digits(sentence):\n",
    "    '''\n",
    "    To handle digits, an approach is used which can be seen with example:\n",
    "    2/1/19 --> date\n",
    "    573568 --> six digit number\n",
    "    3,67,123 --> amount\n",
    "    12:40 --> time\n",
    "    3.14 --> decimal number\n",
    "    \n",
    "    It does exactly what is shown in example. A NLP researcher in his research paper mentioned that using\n",
    "    this approach does no harm to the accuracy of the model and thus used the same approach here.\n",
    "    Although when ran the code with the method, it was seen that accuracy actually increased by 1% on the \n",
    "    test file with Word2Vec embedding.\n",
    "    '''\n",
    "    def to_string(digit):\n",
    "        if x == 1:\n",
    "            return \"one\"\n",
    "        elif x == 2:\n",
    "            return \"two\"\n",
    "        elif x == 3:\n",
    "            return \"three\"\n",
    "        elif x == 4:\n",
    "            return \"four\"\n",
    "        elif x == 5:\n",
    "            return \"five\"\n",
    "        elif x == 6:\n",
    "            return \"six\"\n",
    "        elif x == 7:\n",
    "            return \"seven\"\n",
    "        elif x == 8:\n",
    "            return \"eight\"\n",
    "        elif x == 9:\n",
    "            return \"nine\"\n",
    "        else:\n",
    "            return \"large\"\n",
    "        \n",
    "    pattern = re.compile('.*[0-9].*')\n",
    "    words = sentence.split()\n",
    "    updated_line = \"\"\n",
    "    for word in words:\n",
    "        matched = pattern.match(word)\n",
    "        if matched:\n",
    "            if \",\" in word:\n",
    "                updated_line += \"amount \"\n",
    "            elif \"/\" in word:\n",
    "                updated_line += \"date \"\n",
    "            elif \":\" in word:\n",
    "                updated_line += \"time \"\n",
    "            elif \"-\" in word:\n",
    "                updated_line += \"date \"\n",
    "            elif \".\" in word:\n",
    "                updated_line += \"decimal number \"\n",
    "            else:\n",
    "                x = len(word)\n",
    "                x = to_string(x)\n",
    "                x += \" digit number \"\n",
    "                updated_line += x\n",
    "        else:\n",
    "            word += \" \"\n",
    "            updated_line += word\n",
    "    return updated_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5063870884d7dd3c26821ebdf9f29f024ff5733a"
   },
   "outputs": [],
   "source": [
    "def handle_spelling_errors(sentence):\n",
    "    '''\n",
    "    We need to correct some common mispellings as well. This ensures most of the vocabulary can map itself \n",
    "    to the embedding. The frequency of each mispelling was found using another method later in the code.\n",
    "    '''\n",
    "    spell_correction_dict = {\"qoura\": \"quora\",\n",
    "                            \"qouran\": \"quoran\",\n",
    "                            \"quoracom\": \"quora website\",\n",
    "                            \"wwwyoutubecom\": \"youtube website\",\n",
    "                            \"freelancercom\": \"freelancer website\",\n",
    "                            \"demonitisation\": \"demonetization\",\n",
    "                            \"demonetisation\": \"demonetization\",\n",
    "                            \"bookingcom\": \"booking website\",\n",
    "                            \"upwork\": \"freelancing platform\",\n",
    "                            \"trumpcare\": \"trump care\",\n",
    "                            \"brexit\": \"britain exit from europe\",\n",
    "                            \"iiith\": \"iiit hyderabad\",\n",
    "                            \"cryptocurrencies\": \"multiple cryptocurrency\",\n",
    "                            \"pokémon\": \"pokemon\",\n",
    "                            \"clickbait\": \"forced click\",\n",
    "                            \"naukricom\": \"indian job portal website\",\n",
    "                            \"bhakts\": \"devotees\",\n",
    "                            \"…\": \"\",\n",
    "                             \"etc…\": \"etc\",\n",
    "                             \"π\": \"pi\",\n",
    "                             \"√\": \"square root\",\n",
    "                             \"blockchains\": \"blockchain\",\n",
    "                             \"∞\": \"infinity\"\n",
    "                            }\n",
    "    correct_sentence = \"\"\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            x = spell_correction_dict[word.lower()]\n",
    "            correct_sentence += x\n",
    "        except KeyError:\n",
    "            correct_sentence += word\n",
    "        correct_sentence += \" \"\n",
    "    return correct_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0930ac6540fe714dd431d6d14ea0be046cda8a19"
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    '''\n",
    "    This method calls other methods needed to clean sentence. Some methods are commented and not even\n",
    "    provided with this code because GloVe embedding was last used with it and it has embedding for most of\n",
    "    the proper nouns but not Word2Vec. It is kept such so as to remind that whenever use this code with \n",
    "    Word2Vec, add these methods too to increase accuracy. Handling non-English words is not actually\n",
    "    possible as there are like tens of other languages used in the train data and can't figure out every\n",
    "    language's meaning.\n",
    "    '''\n",
    "    sentence = handle_contractions(sentence)\n",
    "    sentence = handle_digits(sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = handle_punctuations(sentence)\n",
    "    #sentence = handle_non_English_words(sentence)\n",
    "    #sentence = handle_acronyms_and_proper_nouns(sentence)\n",
    "    sentence = handle_spelling_errors(sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2661f5703564ed467652feb3291103b95cb58303"
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(sentence_list):\n",
    "    '''\n",
    "    Here comes interesting part. We create a bag of words. These are all the unique words in the text.\n",
    "    Keeping a count of all the unique words and the frequency of it helps in finding out how much of our\n",
    "    dataset is useful. By useful, it is meant that how much of our dataset can be converted into vectors\n",
    "    provided by GloVe embedding.\n",
    "    Later, vocabulary is sorted by values in reverse order to find out stop words.\n",
    "    It returns the vocabulary dictionary and the updated sentence list which is now cleaned.\n",
    "    '''\n",
    "    new_sentence_list = []\n",
    "    vocabulary = {}\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        sentence = clean_sentence(sentence)\n",
    "        new_sentence_list.append(sentence)\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            try:\n",
    "                vocabulary[word] += 1\n",
    "            except KeyError:\n",
    "                vocabulary[word] = 1\n",
    "    vocabulary = OrderedDict(sorted(vocabulary.items(), key = lambda x:x[1], reverse = True))\n",
    "    return vocabulary, new_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b732b085be344a1b776d3f73cf525bc6d1dadf7"
   },
   "outputs": [],
   "source": [
    "def check_coverage(vocabulary, embedding):\n",
    "    '''This method checks how much of our vocabulary actually has an embedding. A simple way was to \n",
    "    convert keys to set for both vocabulary and glove and then get the intersection of sets using set\n",
    "    operations.'''\n",
    "    words_vocabulary = set(vocabulary.keys())\n",
    "    words_embedding = set(embedding.keys())\n",
    "    intersection = words_vocabulary & words_embedding\n",
    "    print('Found embeddings for {:.2%} of our vocabulary'.format(len(intersection)/len(words_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "614c328da4aa1b601d8153b94750c4e0e12bfaf8"
   },
   "outputs": [],
   "source": [
    "glove_embedding = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "\n",
    "def loading_glove_embedding(glove_embedding):\n",
    "    '''\n",
    "    This method loads the embedding onto the RAM. This feature is costly on RAM but is required as well\n",
    "    to perform the tasks. It takes around 3-4 minutes to load such an embedding as it weighs around 6GB.\n",
    "    '''\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embedding = dict(get_coefs(*o.split(\" \")) for o in open(glove_embedding, encoding='latin'))\n",
    "    return embedding\n",
    "\n",
    "glove = loading_glove_embedding(glove_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create a vocabulary and update the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c159caa6d912baee1c593fe5a40f056f2ff3d60"
   },
   "outputs": [],
   "source": [
    "vocabulary, sentence_list_train = create_vocabulary(sentence_list_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to generalize the code for any embedding. GloVe does understand some words in upper case like proper nouns but not the other embeddings. This method ahead, and many previous ones, just ensure that if we change the embedding, we do not need to modify the code much which was necessary because of difference in the way words are treated by an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "183817869a9978a4bac9592a9f346135cb90b5e2"
   },
   "outputs": [],
   "source": [
    "def to_lower_case(vocabulary):\n",
    "    '''\n",
    "    In this method, we are bringing our words in bag of words to lower case. Now if a word is present twice \n",
    "    in our bag of words, once in upper case and once in lower case, we add up their frequency and delete\n",
    "    the upper case word. Else we bring down the word to lower case without altering the frequency.\n",
    "    Using try-except block helps in the if block. Notice we just access both the word and the lower word\n",
    "    without even checking that lower word previously existed in the vocabulary or not. This might result\n",
    "    in an exception for some cases. For those cases and except block is used. If we were to search a key is \n",
    "    actually present in our bag of words, it could be a costly affair as one can see how big our bag of\n",
    "    words actually is. This way, it ensures complexity of code is O(n).\n",
    "    '''\n",
    "    updated_vocabulary = {}\n",
    "    for word in tqdm(vocabulary):\n",
    "        lower_word = word.lower()\n",
    "        try:\n",
    "            if word != lower_word:\n",
    "                updated_vocabulary[lower_word] = vocabulary[word] + vocabulary[lower_word]\n",
    "            else:\n",
    "                updated_vocabulary[lower_word] = vocabulary[word]\n",
    "        except KeyError:\n",
    "            updated_vocabulary[lower_word] = vocabulary[word]\n",
    "    return updated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40e0eb1a753c60791dd775e4c8c36988c911a8db"
   },
   "outputs": [],
   "source": [
    "def update_glove(vocabulary, glove):\n",
    "    '''\n",
    "    We need to update the embedding too. If embedding for an upper case word exists, but not of the lower\n",
    "    case one, it just adds an embedding for a lower case word same as that of upper case one.\n",
    "    '''\n",
    "    for word in tqdm(vocabulary):\n",
    "        lower_word = word.lower()\n",
    "        if word in glove and lower_word not in glove:\n",
    "            glove[lower_word] = glove[word]\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37c392ef2f49f5e606a0d8ad21f94fc7d350d23f"
   },
   "outputs": [],
   "source": [
    "glove = update_glove(vocabulary, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c0c1f0673f1536be5d43bff5b7fe54bebdc6395"
   },
   "outputs": [],
   "source": [
    "vocabulary = to_lower_case(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc0e8fce50f818162443f68e6d7777f20d58ed04"
   },
   "outputs": [],
   "source": [
    "check_coverage(vocabulary, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "690c679947a899d85c3d1ed505f469ba970af3e5"
   },
   "outputs": [],
   "source": [
    "def create_oov_dictionary(vocabulary, glove):\n",
    "    '''\n",
    "    Now, we need a dictionary which has the words which are in bag of words but not in embedding. So as to\n",
    "    manually add them in our embedding by correcting its spelling, or using some synonym if its frequency\n",
    "    is large enough to impact our model's capacity.\n",
    "    '''\n",
    "    oov_dictionary = {}\n",
    "    for key in tqdm(vocabulary):\n",
    "        try:\n",
    "            x = glove[key]\n",
    "        except KeyError:\n",
    "            oov_dictionary[key] = vocabulary[key]\n",
    "    oov_dictionary = OrderedDict(sorted(oov_dictionary.items(), key = lambda x:x[1], reverse = True))\n",
    "    return oov_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f164ca1a168d581cf384a0c527e7654d250f4056"
   },
   "outputs": [],
   "source": [
    "oov_dict = create_oov_dictionary(vocabulary, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c4739f9cdfffc1f8eccedd6720bb6cc73ac46ba"
   },
   "outputs": [],
   "source": [
    "def lower_case_sentence(sentence_list):\n",
    "    '''\n",
    "    We bring all of the sentences in the list to lower case now. As our bag of words does not recognize\n",
    "    any upper case word anymore.\n",
    "    '''\n",
    "    sentence_list_new = []\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        sentence = sentence.lower()\n",
    "        sentence_list_new.append(sentence)\n",
    "    del sentence_list\n",
    "    return sentence_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1f54abc7593229ea5e5edcf68b4fc516ec9d822"
   },
   "outputs": [],
   "source": [
    "sentence_list_train = lower_case_sentence(sentence_list_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All preprocessing stuff is done. Add the sentence list back to dataframe to get back on standard ways of creating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cef52d8e1c3ff002607c29910b056c8c1dd839f2"
   },
   "outputs": [],
   "source": [
    "train_data['question_text'] = sentence_list_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from here on, we start with our standard algorithm of model building.\n",
    "\n",
    "Extract features and target from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7fcaacfc4482bc4ba942ab8d3edf4a381cdb62f"
   },
   "outputs": [],
   "source": [
    "features = train_data.iloc[:, 1:-1].values\n",
    "target = train_data.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have unlimited RAM, do we? So clear off data we no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "993a69bea05cd13cb573d8f35ad82d752416b2e0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e363fd97258f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0msentence_list_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "del train_data\n",
    "del sentence_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddeca7f301b38fb864c61b08965a2de3b4a2665d"
   },
   "outputs": [],
   "source": [
    "def pretrainedEmbedding():\n",
    "    '''\n",
    "    This method creates an embedding layer for our deep learning model. In this layer our embedding is\n",
    "    brought into the format Keras understands and is added to the layer as weights.\n",
    "    \n",
    "    We make our bag of words and embedding global because once we are done using it here, we no longer need\n",
    "    them and can delete from RAM as this function uses almost 5 GB of RAM and deleting other data that \n",
    "    already occupies 10 GB of our RAM would help save us some space when running by ensuring system doesn't\n",
    "    deadlock.\n",
    "    '''\n",
    "    \n",
    "    global vocabulary\n",
    "    global glove\n",
    "    \n",
    "    def wordToIndex(embed):\n",
    "        tokens = sorted(embed.keys())\n",
    "        wordIndex = {}\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            kerasIdx = idx + 1\n",
    "            wordIndex[tok] = kerasIdx\n",
    "        return wordIndex\n",
    "    \n",
    "    wordIndex = wordToIndex(glove)\n",
    "    \n",
    "    vocabLength = len(wordIndex) + 1\n",
    "    embDim = next(iter(glove.values())).shape[0]\n",
    "    \n",
    "    embeddingMatrix = np.zeros((vocabLength, embDim))\n",
    "    for word, index in tqdm(wordIndex.items()):\n",
    "        embeddingMatrix[index, : ] = glove[word]\n",
    "    \n",
    "    del vocabulary\n",
    "    del glove\n",
    "    \n",
    "    embeddingLayer = Embedding(vocabLength, embDim, weights = [embeddingMatrix], trainable = False)\n",
    "    \n",
    "    return embeddingLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our Sequential model. First layer is always an embedding layer in a NLP model. Next we must use RNN network as the order matters when language processing is done. CuDNNLSTM is used instead of LSTM layer so to increase speed by running our model on GPU instead of RAM. As LSTM is meant to share information with the previous layers, it is kept Bidirectional. The second LSTM layer though doesn't return sequences as the next layer after it is a Dense one which never expects to get any other data other than which the model propagates. Keeping our activation layer as a sigmoid one ensures that our output is either 0 or 1.\n",
    "\n",
    "LSTM --> Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d251ff6cfb96319d5ce4fbe46694564f3f1667a"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(pretrainedEmbedding())\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = True),\n",
    "                        input_shape=(30, 300)))\n",
    "model.add(Bidirectional(CuDNNLSTM(64, return_sequences = False)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to compile our model. Just read the documentation of the parameters passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f852b750e04391508bba150da791ddfbc1458b49"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a model is different than compiling it. Compile just tells it how to do it. Fit tells it on what data it needs to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6007617e92ab96d71f1fb09cd25a60725d9706a3"
   },
   "outputs": [],
   "source": [
    "model.fit(features, target, epochs = 20, batch_size = 512, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Our model is built. Next get our test data file and complete the same preprocessing stuff we did for train dataset. Except, now we do not create a bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fcb51a612f14cef5b4f25180c668a0a07db6c8b5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-baa7ee7ae15d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88e0144e5681468e8d67a0ac18085d2eac00c6b9"
   },
   "outputs": [],
   "source": [
    "test_sentence_list = test_data['question_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f07d5904a28c69337a0294cf08ce721d5b4b185"
   },
   "outputs": [],
   "source": [
    "def update_test_sentences(sentences):\n",
    "    new_sentence_list = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        sentence = clean_sentence(sentence)\n",
    "        new_sentence_list.append(sentence)\n",
    "    return new_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b9bec248deaa0c6201defb3fec7505a87552c3a"
   },
   "outputs": [],
   "source": [
    "test_sentence_list = updated_test_sentences(test_sentence_list)\n",
    "test_sentence_list = lower_case_sentence(test_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f3968e79dac165cfb60084b79d11ba1bab4f53d"
   },
   "outputs": [],
   "source": [
    "test_data['question_text'] = test_sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the question IDs and the questions from the dataframe. We need an output CSV file for submitting in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dcbf5d70aed1d7a84b0b68180e849f95b785da2"
   },
   "outputs": [],
   "source": [
    "ques_id = test_data['qid'].tolist() # for safety reasons\n",
    "questions = test_data.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ec6eecd76df44458d116ba59c6a93c387c7dace"
   },
   "outputs": [],
   "source": [
    "y_predicted = []\n",
    "for question in tqdm(questions):\n",
    "    y_predicted.extend(model.predict(question).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20bcf04438b4297513c303d3eb94094f469dea3c"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"qid\": ques_id, \"prediction\": y_predicted})\n",
    "submission_df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output file is ready. Submit the file and see where we stand on leaderboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
