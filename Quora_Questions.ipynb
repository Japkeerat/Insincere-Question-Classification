{
  "cells": [
    {
      "metadata": {
        "_uuid": "17423878f5d9c2c469700a932b708ed106f0d7ae"
      },
      "cell_type": "markdown",
      "source": "Importing all the packages that are used ahead."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport re\nfrom sklearn.model_selection import train_test_split as tts\nfrom keras.models import Sequential\nfrom keras.layers import Dense, CuDNNLSTM, Embedding, Bidirectional",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "tqdm.pandas()\ntrain_data = pd.read_csv(\"../input/train.csv\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2a8d9a960bddffae7d1a13888b3d8bec45a087f"
      },
      "cell_type": "markdown",
      "source": "Coverting pandas dataframe columns to lists for easier and faster modification."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "123f60c096fd54baa4d02ac6f0f2a04fc7b4365b"
      },
      "cell_type": "code",
      "source": "sentence_list_train = train_data['question_text'].tolist()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b02cbe713a9256472294bf1633ba5349b5252c3b"
      },
      "cell_type": "markdown",
      "source": "Handling punctuations. Some scope of improving time complexity here."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15c81696c0183a728862735430eea17c6cdcbece"
      },
      "cell_type": "code",
      "source": "def handle_punctuations(sentence):\n    sentence = str(sentence)\n    for punct in \"/-'\":\n        sentence = sentence.replace(punct, ' ')\n    for punct in '&':\n        sentence = sentence.replace(punct, ' and ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        sentence = sentence.replace(punct, '')\n    return sentence",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "247d2b5290b4b48e1cc0da24274b80b2db6ded35"
      },
      "cell_type": "markdown",
      "source": "> Source of this dictionary: [Github gist by Neal Shyam](https://gist.github.com/nealrs/96342d8231b75cf4bb82)\n\nThis method expands contractions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6356f37b0391f5a4e5954c4ffe421e7dae99776"
      },
      "cell_type": "code",
      "source": "def handle_contractions(sentence):\n    contraction_dict = {\"ain't\": \"am not\",\n                        \"aren't\": \"are not\",\n                        \"can't\": \"cannot\",\n                        \"can't've\": \"cannot have\",\n                        \"'cause\": \"because\",\n                        \"could've\": \"could have\",\n                        \"couldn't\": \"could not\",\n                        \"couldn't've\": \"could not have\",\n                        \"didn't\": \"did not\",\n                        \"doesn't\": \"does not\",\n                        \"Don't\": \"do not\",\n                        \"don't\": \"do not\",\n                        \"hadn't\": \"had not\",\n                        \"hadn't've\": \"had not have\",\n                        \"hasn't\": \"has not\",\n                        \"haven't\": \"have not\",\n                        \"he'd\": \"he would\",\n                        \"he'd've\": \"he would have\",\n                        \"he'll\": \"he will\",\n                        \"he'll've\": \"he will have\",\n                        \"he's\": \"he is\",\n                        \"how'd\": \"how did\",\n                        \"how'd'y\": \"how do you\",\n                        \"how'll\": \"how will\",\n                        \"how's\": \"how is\",\n                        \"I'd\": \"I would\",\n                        \"I'd've\": \"I would have\",\n                        \"I'll\": \"I will\",\n                        \"I'll've\": \"I will have\",\n                        \"i'm\": \"I am\",\n                        \"I'm\": \"I am\",\n                        \"I've\": \"I have\",\n                        \"isn't\": \"is not\",\n                        \"it'd\": \"it had\",\n                        \"it'd've\": \"it would have\",\n                        \"it'll\": \"it will\",\n                        \"it'll've\": \"it will have\",\n                        \"it's\": \"it is\",\n                        \"let's\": \"let us\",\n                        \"ma'am\": \"madam\",\n                        \"mayn't\": \"may not\",\n                        \"might've\": \"might have\",\n                        \"mightn't\": \"might not\",\n                        \"mightn't've\": \"might not have\", \n                        \"must've\": \"must have\",\n                        \"mustn't\": \"must not\",\n                        \"mustn't've\": \"must not have\",\n                        \"needn't\": \"need not\",\n                        \"needn't've\": \"need not have\",\n                        \"o'clock\": \"of the clock\",\n                        \"oughtn't\": \"ought not\",\n                        \"oughtn't've\": \"ought not have\",\n                        \"shan't\": \"shall not\",\n                        \"sha'n't\": \"shall not\",\n                        \"shan't've\": \"shall not have\",\n                        \"she'd\": \"she would\",\n                        \"she'd've\": \"she would have\",\n                        \"she'll\": \"she will\",\n                        \"she'll've\": \"she will have\",\n                        \"she's\": \"she is\",\n                        \"should've\": \"should have\",\n                        \"shouldn't\": \"should not\",\n                        \"shouldn't've\": \"should not have\",\n                        \"so've\": \"so have\",\n                        \"so's\": \"so is\",\n                        \"that'd\": \"that would\",\n                        \"that'd've\": \"that would have\",\n                        \"that's\": \"that is\",\n                        \"there'd\": \"there had\",\n                        \"there'd've\": \"there would have\",\n                        \"there's\": \"there is\",\n                        \"they'd\": \"they would\",\n                        \"they'd've\": \"they would have\",\n                        \"they'll\": \"they will\",\n                        \"they'll've\": \"they will have\",\n                        \"they're\": \"they are\",\n                        \"they've\": \"they have\",\n                        \"to've\": \"to have\",\n                        \"wasn't\": \"was not\",\n                        \"we'd\": \"we had\",\n                        \"we'd've\": \"we would have\",\n                        \"we'll\": \"we will\",\n                        \"we'll've\": \"we will have\",\n                        \"we're\": \"we are\",\n                        \"we've\": \"we have\",\n                        \"weren't\": \"were not\",\n                        \"what'll\": \"what will\",\n                        \"what'll've\": \"what will have\",\n                        \"what're\": \"what are\",\n                        \"what's\": \"what is\",\n                        \"what've\": \"what have\",\n                        \"when's\": \"when is\",\n                        \"when've\": \"when have\",\n                        \"where'd\": \"where did\",\n                        \"where's\": \"where is\",\n                        \"where've\": \"where have\",\n                        \"who'll\": \"who will\",\n                        \"who'll've\": \"who will have\",\n                        \"who's\": \"who is\",\n                        \"who've\": \"who have\",\n                        \"why's\": \"why is\",\n                        \"why've\": \"why have\",\n                        \"will've\": \"will have\",\n                        \"won't\": \"will not\",\n                        \"won't've\": \"will not have\",\n                        \"would've\": \"would have\",\n                        \"wouldn't\": \"would not\",\n                        \"wouldn't've\": \"would not have\",\n                        \"y'all\": \"you all\",\n                        \"y'alls\": \"you alls\",\n                        \"y'all'd\": \"you all would\",\n                        \"y'all'd've\": \"you all would have\",\n                        \"y'all're\": \"you all are\",\n                        \"y'all've\": \"you all have\",\n                        \"you'd\": \"you had\",\n                        \"you'd've\": \"you would have\",\n                        \"you'll\": \"you you will\",\n                        \"you'll've\": \"you you will have\",\n                        \"you're\": \"you are\",\n                        \"you've\": \"you have\"\n                       }\n    updated_sentence = \"\"\n    words = sentence.split()\n    for word in words:\n        try:\n            updated_sentence += contraction_dict[word]\n        except KeyError:\n            updated_sentence += word\n        updated_sentence += \" \"\n    return updated_sentence",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5969a39742be4df600339109e159ca0482ac2d71"
      },
      "cell_type": "markdown",
      "source": "> Idea of this approach: [An algorithm that learns what is in a name - Research paper(Skip to page 7)](http://people.csail.mit.edu/mcollins/6864/slides/bikel.pdf)\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4417f8957a1faf95c9ef73ef5cc7a3c692b7450"
      },
      "cell_type": "code",
      "source": "def handle_digits(sentence):\n    \n    def to_string(digit):\n        if x == 1:\n            return \"one\"\n        elif x == 2:\n            return \"two\"\n        elif x == 3:\n            return \"three\"\n        elif x == 4:\n            return \"four\"\n        elif x == 5:\n            return \"five\"\n        elif x == 6:\n            return \"six\"\n        elif x == 7:\n            return \"seven\"\n        elif x == 8:\n            return \"eight\"\n        elif x == 9:\n            return \"nine\"\n        else:\n            return \"large\"\n        \n    pattern = re.compile('.*[0-9].*')\n    words = sentence.split()\n    updated_line = \"\"\n    for word in words:\n        matched = pattern.match(word)\n        if matched:\n            if \",\" in word:\n                updated_line += \"amount \"\n            elif \"/\" in word:\n                updated_line += \"date \"\n            elif \":\" in word:\n                updated_line += \"time \"\n            elif \"-\" in word:\n                updated_line += \"date \"\n            elif \".\" in word:\n                updated_line += \"decimal number \"\n            else:\n                x = len(word)\n                x = to_string(x)\n                x += \" digit number \"\n                updated_line += x\n        else:\n            word += \" \"\n            updated_line += word\n    return updated_line",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5063870884d7dd3c26821ebdf9f29f024ff5733a"
      },
      "cell_type": "code",
      "source": "def handle_spelling_errors(sentence):\n    spell_correction_dict = {\"qoura\": \"quora\",\n                            \"qouran\": \"quoran\",\n                            \"quoracom\": \"quora website\",\n                            \"wwwyoutubecom\": \"youtube website\",\n                            \"freelancercom\": \"freelancer website\",\n                            \"demonitisation\": \"demonetization\",\n                            \"demonetisation\": \"demonetization\",\n                            \"bookingcom\": \"booking website\",\n                            \"upwork\": \"freelancing platform\",\n                            \"trumpcare\": \"trump care\",\n                            \"brexit\": \"britain exit from europe\",\n                            \"iiith\": \"iiit hyderabad\",\n                            \"cryptocurrencies\": \"multiple cryptocurrency\",\n                            \"pokémon\": \"pokemon\",\n                            \"clickbait\": \"forced click\",\n                            \"naukricom\": \"indian job portal website\",\n                            \"bhakts\": \"devotees\",\n                            \"…\": \"\",\n                             \"etc…\": \"etc\",\n                             \"π\": \"pi\",\n                             \"√\": \"square root\",\n                             \"blockchains\": \"blockchain\",\n                             \"∞\": \"infinity\"\n                            }\n    correct_sentence = \"\"\n    words = sentence.split()\n    for word in words:\n        try:\n            x = spell_correction_dict[word.lower()]\n            correct_sentence += x\n        except KeyError:\n            correct_sentence += word\n        correct_sentence += \" \"\n    return correct_sentence",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dcd5781c84bae121abcf22db58e2624bf8904b91"
      },
      "cell_type": "code",
      "source": "def handle_non_English_words(sentence):\n    pass",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48d0327b3fb298aef7316993dcf32cfacb71cde6"
      },
      "cell_type": "code",
      "source": "def handle_acronyms_and_proper_nouns(sentence):\n    pass",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65a08a788f44ad00bd5b3e8ace23010723923c68"
      },
      "cell_type": "markdown",
      "source": "As name suggests, this method is used to clean sentences."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0930ac6540fe714dd431d6d14ea0be046cda8a19"
      },
      "cell_type": "code",
      "source": "def clean_sentence(sentence):\n    sentence = handle_contractions(sentence)\n    sentence = handle_digits(sentence)\n    sentence = sentence.strip()\n    sentence = handle_punctuations(sentence)\n    #sentence = handle_digits_and_nouns(sentence)\n    #sentence = handle_non_English_words(sentence)\n    #sentence = handle_acronyms_and_proper_nouns(sentence)\n    sentence = handle_spelling_errors(sentence)\n    sentence = sentence.strip()\n    return sentence",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7db6fbb20c0e21fa15f960b9f788c8058d86a02"
      },
      "cell_type": "markdown",
      "source": "This method creates vocabulary of ours. If word is already in our dictionary, add 1 to the value. Else add the word with value initialized to 1. Then using OrderedDict, it is sorted in reverse order."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2661f5703564ed467652feb3291103b95cb58303"
      },
      "cell_type": "code",
      "source": "def create_vocabulary(sentence_list):\n    new_sentence_list = []\n    vocabulary = {}\n    for sentence in tqdm(sentence_list):\n        sentence = clean_sentence(sentence)\n        new_sentence_list.append(sentence)\n        words = sentence.split()\n        for word in words:\n            try:\n                vocabulary[word] += 1\n            except KeyError:\n                vocabulary[word] = 1\n    vocabulary = OrderedDict(sorted(vocabulary.items(), key = lambda x:x[1], reverse = True))\n    return vocabulary, new_sentence_list",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf52bb342dbc23393b6180405f0c7fd8f0741ab3"
      },
      "cell_type": "markdown",
      "source": "This method finds how much of our vocabulary is actually useful."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b732b085be344a1b776d3f73cf525bc6d1dadf7"
      },
      "cell_type": "code",
      "source": "def check_coverage(vocabulary, embedding):\n    words_vocabulary = set(vocabulary.keys())\n    words_embedding = set(embedding.keys())\n    intersection = words_vocabulary & words_embedding\n    print('Found embeddings for {:.2%} of our vocabulary'.format(len(intersection)/len(words_vocabulary)))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e09b571f7e0b0dc5bf5ddcf0f1bf54c98db90ec0"
      },
      "cell_type": "markdown",
      "source": "> Source of this function to load embedding: [Theo Viel's Notebook](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "614c328da4aa1b601d8153b94750c4e0e12bfaf8"
      },
      "cell_type": "code",
      "source": "glove_embedding = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n\ndef loading_glove_embedding(glove_embedding):\n    \n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    model = dict(get_coefs(*o.split(\" \")) for o in open(glove_embedding, encoding='latin'))\n    return model\n\nglove = loading_glove_embedding(glove_embedding)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c159caa6d912baee1c593fe5a40f056f2ff3d60"
      },
      "cell_type": "code",
      "source": "vocabulary, sentence_list_train = create_vocabulary(sentence_list_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb04110e83acd98093ae939bec99df8186748d20"
      },
      "cell_type": "markdown",
      "source": "This method brings all elements of our vocabulary to lower case. If lower case element is already present, add both lowercase and uppercase values, else only the one.  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "183817869a9978a4bac9592a9f346135cb90b5e2"
      },
      "cell_type": "code",
      "source": "def to_lower_case(vocabulary):\n    updated_vocabulary = {}\n    for word in tqdm(vocabulary):\n        lower_word = word.lower()\n        try:\n            if word != lower_word:\n                updated_vocabulary[lower_word] = vocabulary[word] + vocabulary[lower_word]\n            else:\n                updated_vocabulary[lower_word] = vocabulary[word]\n        except KeyError:\n            updated_vocabulary[lower_word] = vocabulary[word]\n    return updated_vocabulary",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f1daf937ed1c1e9d9e321bf3470f76d4b313efb"
      },
      "cell_type": "markdown",
      "source": "Updating the embedding. Some words are known as proper nouns in Glove while others keep it small. Therefore, add lower case words to the embedding with their values initialized to the uppercase words."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40e0eb1a753c60791dd775e4c8c36988c911a8db"
      },
      "cell_type": "code",
      "source": "def update_glove(vocabulary, glove):\n    print(len(glove))\n    for word in tqdm(vocabulary):\n        lower_word = word.lower()\n        if word in glove and lower_word not in glove:\n            glove[lower_word] = glove[word]\n    print(len(glove))\n    return glove",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37c392ef2f49f5e606a0d8ad21f94fc7d350d23f"
      },
      "cell_type": "code",
      "source": "glove = update_glove(vocabulary, glove)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c0c1f0673f1536be5d43bff5b7fe54bebdc6395"
      },
      "cell_type": "code",
      "source": "vocabulary = to_lower_case(vocabulary)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc0e8fce50f818162443f68e6d7777f20d58ed04"
      },
      "cell_type": "code",
      "source": "check_coverage(vocabulary, glove)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "690c679947a899d85c3d1ed505f469ba970af3e5"
      },
      "cell_type": "code",
      "source": "def create_oov_dictionary(vocabulary, glove):\n    oov_dictionary = {}\n    for key in tqdm(vocabulary):\n        try:\n            x = glove[key]\n        except KeyError:\n            oov_dictionary[key] = vocabulary[key]\n    oov_dictionary = OrderedDict(sorted(oov_dictionary.items(), key = lambda x:x[1], reverse = True))\n    return oov_dictionary",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "842defef986448193530e0f9403a18b6f381b008"
      },
      "cell_type": "code",
      "source": "oov_vocabulary = create_oov_dictionary(vocabulary, glove)\nprint({k:oov_vocabulary[k] for k in list(oov_vocabulary)[:100]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c4739f9cdfffc1f8eccedd6720bb6cc73ac46ba"
      },
      "cell_type": "code",
      "source": "def lower_case_sentence(sentence_list):\n    sentence_list_new = []\n    for sentence in tqdm(sentence_list):\n        sentence = sentence.lower()\n        sentence_list_new.append(sentence)\n    return sentence_list_new",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1f54abc7593229ea5e5edcf68b4fc516ec9d822"
      },
      "cell_type": "code",
      "source": "sentence_list_train = lower_case_sentence(sentence_list_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cef52d8e1c3ff002607c29910b056c8c1dd839f2"
      },
      "cell_type": "code",
      "source": "col = train_data.columns[1]\ntrain_data[col] = sentence_list_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7fcaacfc4482bc4ba942ab8d3edf4a381cdb62f"
      },
      "cell_type": "code",
      "source": "features = np.asarray(train_data['question_text'].tolist())\ntarget = np.asarray(train_data['target'].tolist())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b0a0255e56d74475575f80623d2deb4324a719e"
      },
      "cell_type": "code",
      "source": "x_train, x_validate, y_train, y_validate = tts(features, target, test_size = 0.2, random_state = 42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2ace985990bbd7eb5c548e3ced60d29a724d823"
      },
      "cell_type": "code",
      "source": "del train_data\ndel oov_vocabulary\ndel sentence_list_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62ff226daa757280e4d988459d03a6d242f1ab07"
      },
      "cell_type": "code",
      "source": "def pretrainedEmbedding(vocab, embed):\n    \n    def wordToIndex(embed):\n        tokens = sorted(embed.keys())\n        wordIndex = {}\n        for idx, tok in enumerate(tokens):\n            kerasIdx = idx + 1\n            wordIndex[tok] = kerasIdx\n        return wordIndex\n    \n    wordIndex = wordToIndex(embed)\n    \n    vocabLength = len(wordIndex) + 1\n    embDim = next(iter(embed.values())).shape[0]\n    \n    embeddingMatrix = np.zeros((vocabLength, embDim))\n    for word, index in tqdm(wordIndex.items()):\n        embeddingMatrix[index, : ] = embed[word]\n    \n    embeddingLayer = Embedding(vocabLength, embDim, weights = [embeddingMatrix], trainable = False)\n    return embeddingLayer",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d58714178d4754a7e274302871be5b656e19f8d9"
      },
      "cell_type": "code",
      "source": "model = Sequential()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d251ff6cfb96319d5ce4fbe46694564f3f1667a"
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(pretrainedEmbedding(vocabulary, glove))\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n                        input_shape=(30, 300)))\nmodel.add(Bidirectional(CuDNNLSTM(64)))\nmodel.add(Dense(1, activation=\"sigmoid\"))",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8b3bd9b0c358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrainedEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n\u001b[1;32m      4\u001b[0m                         input_shape=(30, 300)))\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-351c725b35f8>\u001b[0m in \u001b[0;36mpretrainedEmbedding\u001b[0;34m(vocab, embed)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0membDim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0membeddingMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabLength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membDim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membeddingMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f852b750e04391508bba150da791ddfbc1458b49"
      },
      "cell_type": "code",
      "source": "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6007617e92ab96d71f1fb09cd25a60725d9706a3"
      },
      "cell_type": "code",
      "source": "model.fit_generator(x_train, y_train, epochs = 20, steps_per_epoch = 1000 , validation_set = [x_validate, y_validate], verbose = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b275e33a703c8088d7ea877e644267dd1e191bc"
      },
      "cell_type": "code",
      "source": "del vocabulary\ndel glove\ntest_data = pd.read_csv('../input/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b0a90a5892d3d8d3692bec6e16f23c32a2c6d3a"
      },
      "cell_type": "code",
      "source": "test_sentence_list = test_data['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12ac7edc46148d4cb8a7aba1510bab7f40a0e8c8"
      },
      "cell_type": "code",
      "source": "def update_test_sentences(sentences):\n    new_sentence_list = []\n    for sentence in tqdm(sentences):\n        sentence = clean_sentence(sentence)\n        new_sentence_list.append(sentence)\n    return new_sentence_list",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f42bb41c06d168a3b5f2564a9b2ec284ab0f8a4"
      },
      "cell_type": "code",
      "source": "test_sentence_list = updated_test_sentences(test_sentence_list)\ntest_sentence_list = lower_case_sentence(test_sentence_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39e879cb9a47ce25fb7c7a689c6466b0a6a1bec5"
      },
      "cell_type": "code",
      "source": "test_data['question_text'] = test_sentence_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56752aced2d2cf5eb7143409a5d15a78bb27e9a4"
      },
      "cell_type": "code",
      "source": "ques_id = test_data['qid'].tolist() # for safety reasons\nquestions = np.asarray(test_data['question_text'].tolist())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fda6fb505c06e6852e2ea9cc06d425cf66117092"
      },
      "cell_type": "code",
      "source": "questions = text_to_number(questions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4286c20a494291fe310e4beecd7738f41e4d67dd"
      },
      "cell_type": "code",
      "source": "y_predicted = []\nfor question in tqdm(questions):\n    y_predicted.extend(model.predict(question).flatten())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0948be6f7ad0f98939316edb04d82ed07e952482"
      },
      "cell_type": "code",
      "source": "submission_df = pd.DataFrame({\"qid\": ques_id, \"prediction\": y_predicted})\nsubmission_df.to_csv(\"submission.csv\", index = False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
